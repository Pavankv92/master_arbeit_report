\section{Point cloud for scan registration} As previously discussed, point cloud for scan registration is generated by combining 2D pixel values from YOLO detection and depth value from kinect as shown in . Middle point of the YOLO bounding box is taken as pixel value and corresponding depth value is retrieved to form 3D point cloud. A single electrode can be seen from multiple adjacent images. Since middle point of the bounding box is taken as the pixel value slight variation in the bounding box position has an effect in addition to the out-projection error \cref{fig:yolo_performance}.

\begin{figure}[hbt!]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\linewidth]{yolo_detection_2.jpg}	
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.42\textwidth}
		\centering
		\includegraphics[width=\linewidth]{yolo_detection_depth_2.jpg}	
	\end{subfigure}
	\caption{3D point cloud generated from YOLO detection and depth.}
	\label{fig:yolo_depth_point_cloud} 
\end{figure}  
\begin{figure}[hbt!]
	\centering
	\includegraphics[scale=0.8]{yolo_performance.png}
	\caption{YOLO bounding box position variation for same electrode from different images.}
	\label{fig:yolo_performance}
\end{figure}

The \cref{fig:camera_scans} depicts hypothetical camera position along the trajectory while collecting the RGB-D images of the phantom head. Note that all the camera position is expressed in robot base frame in case of robot guided trajectory and in tracking camera frame in case of hand guided trajectory. Only few electrodes are visible at each camera position and same electrode can be seen from multiple adjacent images due to high frame rate (30 fps). A unique data structure is used to save camera position and corresponding 3D point cloud of visible electrodes from that position for further processing.

\begin{figure}[hbt!]
	\centering
	\includegraphics[scale=0.8]{camera_scans.png}
	\caption{Fewer camera position is shown for clarity.}
	\label{fig:camera_scans}
\end{figure}

\pagebreak


