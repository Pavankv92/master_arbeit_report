\section{Data acquisition}

\subsection{Robot guided trajectory} Although robot-guided data set is available as a part of the pipeline, the important aspects will be discussed here. As shown in  \cref{fig:robotic_data_acquisition}  setup consists of 2 cameras (RGB-D and a tracking camera), a robot, a phantom head with EEG cap. The Kinect is mounted directly on the end-effector of the robot while the tracking camera is rigidly mounted to the ceiling (Cambar B2 is shown in the graphic instead of fusionTrac 500). The position of the tracking camera from robot base $\tfMat{Base}{T}{tCam}$ is determined using hand-eye calibration. Since the Kinect is mounted directly on the end-effector, a variant form of hand-eye calibration known as eye-in-hand calibration is performed to calculate the position of Kinect from robot base $\tfMat{Base}{T}{kCam}$. First, all the electrodes are digitized using a reflective marker and fusionTrac. Then the robot is guided around the phantom head on a circular trajectory while RGB and depth images are captured using the point and shoot method. The process is repeated several times with the same circular trajectory each time with a re-positioned phantom head. 

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\linewidth]{robotic_data_acquisition.png}
	\caption{Robot-guided data acquisition system.}
	\label{fig:robotic_data_acquisition}
\end{figure}

\subsection{Hand guided trajectory} As shown in \cref{fig:hand_data_acquisition}  setup consists of a tracking camera and an RGB-D camera as in the robot-guided setup. However, a reflective marker is attached to Kinect in order to be tracked from the tracking camera in the absence of a robot. The Kinect is hand-guided around the phantom head trying to maintain a circular trajectory while continuously capturing RGB and depth images at 30fps. The process is repeated several times with different trajectories each time and recorded as ROS .bag files for later processing. 


\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\linewidth]{hand_data_acquisition.png}
	\caption{Camera is hand guided around the phantom head.}
	\label{fig:hand_data_acquisition}
\end{figure}

One of the limitations during hand-guided data acquisition is the limited field of view (FOV) of the tracking camera. \cref{fig:fov_tracking_camera} shows an approximate radius of the phantom head is \textit{R} $\approx$ 70mm and recommended distance for depth is 250 - 2210 mm for azure Kinect in $WFOV\ 2x2\ binned$ mode. Hence the hand-guided radius is maintained $>$ 350mm. However, due to the limited FOV of the tracking camera, only a portion of the trajectory is recorded. A top view of an example trajectory is shown in \cref{fig:cut_trajectory} where it starts from 1 and ends at 6. The segments 1-2, 3-4 and 5-6 are seen while 2-3, 4-5, 6-1 are not, $nav\_msgs/Path$ message type from ROS automatically fills these gaps with straight lines. One of the consequences is that full trajectory is not available for comparison. Therefore, only visible segments are used for trajectory comparison and other metrics.

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\linewidth]{fov_tracking_camera.png}
	\caption{Hand-guided radius is $>$ 350mm.}
	\label{fig:fov_tracking_camera}
\end{figure}

\begin{figure}[hbt!]
	\centering
	\includegraphics[scale=0.6]{cut_trajectory.png}
	\caption{An example for ground truth hand guided trajectory (1-6).}
	\label{fig:cut_trajectory}
\end{figure}


\subsection{Eye-in-hand calibration for hand guided trajectory}
In this section, we provide details to estimate the transformation between the reflective maker and the Kinect coordinate system. The only difference between the setup presented in the general hand-eye calibration and the hand-guided system is the absence of a robot. however, the \cref{eq:hand-eye_hand_data} is still holds with  $\tfMat{tCam}{T}{marker}$ as M,  $\tfMat{marker}{T}{kCam}$ as X,  $\tfMat{tCam}{T}{CB}$ as Y and  $\tfMat{CB}{T}{kCam}$ as N as shown in \cref{fig:handeye_calibration_hand_data}. 

\begin{equation}
	\matr{M}_i\matr{X} - \matr{Y}\matr{N}_i = 0 
	\label{eq:hand-eye_hand_data}
\end{equation}

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\linewidth]{handeye_calibration_hand_data.png}
	\caption{Eye-in-hand calibration for hand guided setup. Coordinate frames are arbitrarily shown for illustration purposes only.}
	\label{fig:handeye_calibration_hand_data}
\end{figure}

The tracking camera and chessboard were static while kinect moved to different position $M_i$ and corresponding chessboard pose is estimated $N_i$. 

\subsection{Time synchronization}

The Cambar B2 communication server,  pose tracking software, and ROS, Kinect software run on two separate computers. The cambar B2 software provides a time stamp in milliseconds relative to starting time of the camera (does not use system time as basis) with alternating 62.5 and 66.66 fps while Kinect data is recorded at 30 fps with ROS timestamps as shown in \cref{fig:time_sync}. Although data acquisition on both computers was manually started together, time delay from pressing the start button to actual starting is expected. As both cameras do not have a common  time as a reference, the first entry of both cameras my not corresponds to each other. Therefore, time synchronization is necessary. The first task is to determine which Cambar entry corresponds to the first entry of Kinect. This may prompt to skip the first few entries from the Cambar data. Once the correct entry is found, all the other time stamps will be expressed relative to just found entry. Then, at time \textit{t}, time stamps both Kinect and Cambar will be matched. If the Kinect pose recorded in Cambar at the exact time is not available, the pose closest to time \textit{t} is chosen. In order to choose the correct first entry in Cambar data, we first map the point clouds using Kinect poses and iterate over first 60 frames (assuming time delay is not more than 1 second), at each frame, the cluster centers, least value of cluster inertia as per \cref{eq:cluster_center_inertia}(sum of distance squared) along with the corresponding entry in the Cambar is stored. Since the change in Kinect raw depth value also affects cluster inertia, later two optimizations will be performed together (details to follow). The \cref{fig:cs_301_red_14_time_sync} shows an extreme case where the  first 48 frames had to be skipped to achieve the desired goal.

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\linewidth]{time_sync.png}
	\caption{left: Kinect data consists of time stamp and point cloud, center : Cambar data consists of time stamp and pose of a marker attached to Kinect (hence pose of Kinect), right: Cambar time stamps relative to first entry.}
	\label{fig:time_sync}
\end{figure}

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\linewidth]{cs_301_red_14_time_sync.png}
	\caption{Time synchronization using raw Kinect depth value.}
	\label{fig:cs_301_red_14_time_sync}
\end{figure}





