In this section, we provide detailed results on various aspects of the thesis.

\section{Camera calibration} 
Several images at different depth in steps of $\approx$ 200mm starting $\approx$ 400mm from the camera were used for camera calibration. The estimated camera intrinsic parameters using OpenCV, Matlab along with values in the robot-guided data set is provided in \cref{tab:kinect_camera_calibration_result}. Image size 1280$\times$720. 

\begin{table}[hbt!]
	\centering
	\resizebox{\textwidth}{!}{
	\begin{tabular}{|c|c|c|c|}
		\hline
		Parameters & Calibrated values (OpenCV) & Calibrated values (Matlab) & Value in robot-guided data set\\ 
		\hline
		$\alpha_x$  & \num{601.06} & \num{604.58}  & \num{607.49}\\
		$\alpha_y$ & \num{601.33} & \num{604.48} & \num{607.44}\\
		S & 0.0 & 0.0 & 0.0\\
		$C_x$  & \num{636.54} & \num{639.25}  & \num{638.92}\\
		$C_y$  & \num{360.29} & \num{358.54}  & \num{364.35}\\
		\hline
	\end{tabular}}
	\caption{Microsoft azure kinect calibration result}
	\label{tab:kinect_camera_calibration_result}
\end{table}

\section{Hand-eye and Eye-in-hand calibration}
For eye-in-hand calibration (hand guided case) the kinect along with firmly attached marker were moved to $>$40 different position making sure that tracking camera can detect the marker and estimate the pose correctly. The collected data set is divided as training (60\%) and evaluation sets (40\%). We randomly choose training data and calculate errors based on evaluation data set and report (Mean$\pm$STD Deviation) separately for translation and rotation. The \cref{tab:handeye_result} shows errors for robot-guided and hand guided case.

\begin{table}[hbt]
	\centering
	\resizebox{\textwidth}{!}{
	\begin{tabular}{|c|c|c|c|}
		\hline
		Camera & Translation error (mm) & Rotational error ($\deg$) & Between\\ 
		\hline
		Kinect & 8.8$\pm$ 0.2  & 0.7682 $\pm$ 0.0204 & Marker-Kinect\\
		
		Tracking camera & 0.18$\pm$ \num{4.5e-06 } & 0.093 $\pm$ \num{5.4e-05 } & Robot-Tracking Camera\\
		
		Kinect & 1.28$\pm$ \num{2.1e-04 }  & 0.211 $\pm$ \num{2.1e-04} & Robot-Kinect\\
		\hline
	\end{tabular}}
	\caption{Hand-eye, Eye-in-hand calibration results}
	\label{tab:handeye_result}
\end{table}

\section{Pixel points out-projection}

For pixel points out-projection experiment, several images of the chessboard at different depths in steps of $\approx$ 200mm starting $\approx$ 400mm from the camera and at each depth, different orientation were captured. Firstly, ground truth object points are calculated in camera coordinates and corresponding image points are out-projected using raw depth provided by kinect. For each image, out-projected values are then compared to the ground truth values with L2 norm as the metric. Since there are many images at a particular depth distance, L2 norm will be averaged over all the images. The \cref{fig:pixel_value_out_projection} shows the results.

\begin{figure}[hbt]
	\centering
	\includegraphics[scale=0.8]{pixel_value_out_projection.png}
	\caption{Overview of mean L2 norm with different distance from the camera.}
	\label{fig:pixel_value_out_projection}
\end{figure}


\section{Depth offset determination}

For robot-guided case, first at each camera position $x_{1:t} $ both ground truth and out-projected point clouds are known (only those electrodes that are visible from a particular camera pose) and an iterative closest point registration can be performed and output RMSE (root mean square error) can be used as a metric and averaged over all camera poses. Second, one can stitch the point clouds and find the cluster centers and compare them to the ground truth values and measure L2 norm. For the ICP scan registration, $\beta$ is varied from 10 to 29 mm (i.e. out-projected points will be raw kinect depth -  $\beta$) brute forcing with the step of 1 mm. The \cref{fig:icp_depth_offset} shows result for 6 robot guided trajectories (2/cap, for 3 caps). Similarly, with same $\beta$ variation for cluster center comparison, \cref{fig:cluster_depth_offset} shows the results. 

\begin{figure}[hbt]
	\centering
	\includegraphics[width=\linewidth]{depth_offset_robot_no_yolo.png}	
	\caption{mean RMSE vs depth offset $\beta$.}
	\label{fig:icp_depth_offset} 
\end{figure} 

\begin{figure}[hbt]
	\centering
	\includegraphics[width=\linewidth]{depth_offset_robot_cluster_no_yolo.png}	
	\caption{mean L2 norm vs depth offset $\beta$.}
	\label{fig:cluster_depth_offset} 
\end{figure} 

K-Means aims to determine the centroids $\mu_j$ that minimizes the inertia defined by  \cref{eq:cluster_center_inertia}. The effect of depth offset can also be seen in inertia estimated by k-means. The \cref{fig:cluster_inertia_vs_beta} shows inertia at different $\beta > 20$ values. Effect of $\beta$ on the point cloud can be visualized in \cref{fig:depth_offset_beta_variations}.

\begin{figure}[hbt]
	\centering
	\includegraphics[width=\linewidth]{cluster_inertia_vs_beta.png}
	\caption{Average cluster Inertia for each trajectory is calculated, note that, inertia is not a normalized metric. Lower the better and zero is optimum}
	\label{fig:cluster_inertia_vs_beta}
\end{figure}

\begin{figure}[hbt]
	\centering
	\includegraphics[width=\linewidth]{depth_offset_beta_variations.png}
	\caption{Effect of change in $\beta$ on point cloud.}
	\label{fig:depth_offset_beta_variations}
\end{figure}

For hand-guided case, optimum $\beta$ is found by combining time synchronization as it also effects the cluster inertia. Therefore, For combined optimization, $\beta$ is varied from 0 to 29 mm and skip is varied from 0 to 60. Where $\beta = 0 $ refers to direct use of Kinect raw depth value $ skip = 0 $ means first Cambar entry corresponds to first Kinect entry. At each iteration of $\beta$, we first solve time synchronization problem and note the lowest inertia and skip values.
The \cref{fig:depth_and_time_sync} shows result of the combined optimization. Numbers in the parenthesis is the average skip value at  which minimum inertia was achieved. On the other hand, higher error in static transform estimation between marker and Kinect led to higher error in absolute 3D position of estimated cluster centers and ground truth electrodes. Therefore, one more ICP between them was inevitable. Cluster center comparison is shown in \cref{fig:depth_offset_hand_cluster}. 

\begin{figure}[hbt]
	\centering
	\includegraphics[width=\linewidth]{depth_and_time_sync.png}
	\caption{Depth and time synchronization.}
	\label{fig:depth_and_time_sync}
\end{figure}


\begin{figure}[hbt!]
	\centering
	\includegraphics[scale=0.8]{depth_offset_hand_cluster_1.png}
	\\
	\includegraphics[scale=0.8]{depth_offset_hand_cluster_2.png}
	\caption{mean L2 norm vs depth offset $\beta$. Bottom: inconsistency in number of clusters estimation}
	\label{fig:depth_offset_hand_cluster}
\end{figure}




