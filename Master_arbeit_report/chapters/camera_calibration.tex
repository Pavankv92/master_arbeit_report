\section{Camera calibration and pixel value out-projection with chessboard}

\subsection{Camera calibration}
Camera calibration is carried out using open-source computer vision library OpenCV and using readily available calibration app from Matlab. Camera calibration is based on 2D/2D point correspondences with the chessboard as a 2D planar object. The homography is calculated using square corners of a chessboard (world points) and its image points. OpenCV needs an arrays of world points, image points,  and grid size of the chessboard (in our case its 5 rows, 8 columns). Several images at different depth in steps of $\approx$ 200mm starting $\approx$ 400mm from the camera were captured and an array of world points (x,y) location of chessboard corners [(0,0), (40,0), (80,0)...] was fed to the algorithm. OpenCV automatically detects these chessboard corners from the images refines them accordingly. 

\begingroup\makeatletter\def\@currenvir{verbatim}
\verbatim
cv2.calibrateCamera(object_points, image_points, ...)
output: rms, camera_matrix, dist_coeffs, rot_vecs, trans_vecs
\end{verbatim}

The OpenCV function cv2.calibrateCamera takes in these world points (object points), image points and some more arguments then outputs geometric error of reprojection (rms), intrinsic parameters (camera\_matrix) distortion coefficients (dist\_coeffs) and extrinsic parameters $(rot\_vecs, trans\_vecs)$.  Matlab on the other hand requires only single square size of the chess board along with images.

\subsection{Chessboard pose estimation}
Having completed the camera calibration we can now make use of the intrinsic parameters and the distortion coefficients as an input to the pose estimation algorithm provided by OpenCV. 

\begingroup\makeatletter\def\@currenvir{verbatim}
\verbatim
cv2.solvePnP(object_points, image_points, intr_mat, dist_coeffs)
output: rot_vecs, trans_vecs 
\end{verbatim}


The OpenCV function cv2.solvePnP takes object points, image points, intrinsic matrix, and distortion coefficients as the arguments and computes rotation and translation vectors. Rotation vector can be converted to 3$\times$3 rotational matrix using cv2.Rodrigues function provided by OpenCV. By combining rotation matrix and translation vector we can form 4$\times$4 homogeneous matrix for further manipulation. One can perform sanity check on the pose estimation using output of cv2.solvePnP. We pretend that image points for chessboard pose is unknown for which output of the cv2.solvePnP is known. Image points can be calculated back using OpenCV function cv2.ProjectPoints and obtained points can be projected on a image for visual sanity check as shown in \cref{fig:pose_estimation_sanity_check}. 

\begingroup\makeatletter\def\@currenvir{verbatim}
\verbatim
cv2.ProjectPoints(object_points,rot_vecs,trans_vecs,intr_mat,dist_coeffs)
output: image_points 
\end{verbatim}


\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\linewidth]{pose_estimation_sanity_check.png}
	\caption{Chessboard visual sanity check on three different depth values.}
	\label{fig:pose_estimation_sanity_check}
\end{figure}


\subsection{Pixel points out-projection}
As shown in \cref{eq:out_projection} pixel values out-projected to form 3D points ($X_c, Y_c, Z_c$) directly proportional to depth and indirectly proportional to estimated focal length of the camera. Depth value obtained from a TOF camera depends on various factors such as, reflectivity of the material, lighting conditions, etc. Kinect has a random error standard deviation $\le$ 17mm, typical systematic error < 11mm + 0.1\% of distance without multi-path interference. Having finished the camera calibration, a chessboard can be used to observe the above phenomena. Several images of the chessboard at different depths in steps of $\approx$ 200mm starting $\approx$ 400mm from the camera were captured. At each depth distance, images with different orientation of chessboard at 30fps were captured.  Firstly, for each image, chessboard pose is estimated using OpenCV solvePnP method and ground truth object points are calculated in camera coordinates. Secondly, corresponding image points are out-projected using raw depth value ($\lambda$) from the kinect as shown in \cref{fig:pixel_value_est_gt}. 

\begin{figure}[hbt!]
	\centering
	\begin{subfigure}{\textwidth}
		\includegraphics[width=\linewidth]{out_projection_600.png}
	\end{subfigure}
	\\
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{out_projection_1800.png}	
	\end{subfigure}
	\caption{Out-projection error between ground truth chessboard corners and estimated using raw kinect depth at 600 and 1600mm from the camera. Vertical axis the distance from the camera.}
	\label{fig:pixel_value_est_gt} 
\end{figure}  

For each image, out-projected values (estimated object points) are then compared to the ground truth values with L2 norm as the metric. Since there are many images at a particular depth distance, L2 norm will be averaged over all the images. Refer results section for more details. 
