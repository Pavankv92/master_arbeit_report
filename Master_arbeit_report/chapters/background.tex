\section{Camera fundamentals}
A gentle introduction to the pinhole camera model, projection matrix, and calibration (internal and external) will be presented in this section. The underlying mathematics and equations are referenced from \cite{OpenCV},  \cite{Hartley:2003:MVG:861369}.

\begin{figure}[hbt!]
	\centering
	%\includegraphics[width=\linewidth]{perspective_projection_3.png}
	\includegraphics[scale=0.5]{perspective_projection_4.png}
	\caption{Pinhole camera geometry. $\vect{F_c}$ \textit{is the camera center,} \textbf{$(C_x, C_y)$} \textit{is the principal point,} $\vect{P}$ \((X, Y, Z)^T\)  \textit{is world point,} $\vect{P_c}$ \((X_c, Y_c, Z_c)^T\) \textit{is world point measured in camera coordinate system.\cite{OpenCV}} }
	\label{fig:perspective projection}
\end{figure}

A simplified perspective projection is shown in \cref{fig:perspective projection}. A world point \( \displaystyle (X_c, Y_c, Z_c)^T\)  is mapped to \( ( \textit{f}\frac{X_c}{Z_c}, \textit{f}\frac{Y_c}{Z_c}, \textit{f} )^T \) on the image plane placed at a focal length distance \textit{f} from the camera center $(F_c)$. Two key facts can be derived from the above projection equation that farther the object is (larger the $Z_c$) from the camera smaller the size in image plane (shrinking operation), these shrunk points are magnified by the focal length \textit{f} to be placed on the image plane. World points are first shrunk \( (x, y)^T =  (\frac{X_c}{Z_c}, \frac{Y_c}{Z_c})^T \) and then magnified \( ( \textit{f}\frac{X_c}{Z_c}, \textit{f}\frac{Y_c}{Z_c}, \textit{f} )^T.\)

Often, the world points are represented in homogeneous coordinates due to several advantages, the ability to express infinite quantities is one of them. The linear mapping between world and image points is evident in the homogeneous representation as shown below.\\ 

\begin{equation}
	Z_c \left[
	\begin{array}{c} x\\ y\\ 1 \end{array} 
	\right] = 
	\begin{bmatrix}
		\textit{f} & 0 & 0 & 0 \\
		0 & \textit{f} & 0 & 0 \\
		0 & 0 & 1 & 0
	\end{bmatrix} 
	\left[
	\begin{array}{c} X_c\\ Y_c\\ Z_c\\ 1 \end{array} 
	\right]
	\label{eq:mat_image_t_camera}
\end{equation}

The linear mapping can be expressed in a compact form $ \vect{x} = \matr{P} \vect{X_c} $ where $\vect{x}$ is a vector of image points, $\vect{X}$ is a vector of world points and $\matr{P}$ is a 3x4 homogeneous matrix called camera projection matrix. In general, the origin of the image plane may not coincide with the principal point, therefore it is necessary to map the projected points to pixels before using the image for further use as show in \cref{fig:mm_to_pixel}.

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\linewidth]{mm_to_pixel.png}
	\caption{Pixel coordinates $(u,v)$ and the camera coordinates $(x, y)$.}
	\label{fig:mm_to_pixel}
\end{figure}

The equation \ref{eq:mm_to_pixel} depicts mapping image to pixel coordinates. where $(S_x)$ is size of pixel width and  $(S_y)$ is size of pixel height. \\
\begin{equation}
	\begin{split}
		(u-C_x) = \frac{x}{S_x}\\
		(v-C_y) = \frac{y}{S_y}
		\label{eq:mm_to_pixel}
	\end{split}
\end{equation}

Therefore a 3D world point \((X_c, Y_c, Z_C)^T\) is mapped to \( ( \textit{f}\frac{X}{Z}+C_x, \textit{f}\frac{Y}{Z}+C_y, \textit{f} )^T \) to the pixel coordinates \( (u, v, z) \) and can be expressed as a matrix multiplication. Where $\alpha_x$ is $(\frac{\textit{f}}{S_x})$ , $\alpha_y$ is $(\frac{\textit{f}}{S_y})$ and S' is slant factor, when the image plane is not normal to the optical axis.

\begin{equation}
	\left[ 
	\begin{array}{c} u\\ v\\ 1 \end{array} 
	\right] = 
	\begin{bmatrix}
		\frac{1}{S_x} & S' & C_x \\
		0 & \frac{1}{S_y} & C_y \\
		0 & 0 & 1
	\end{bmatrix} 
	\left[ 
	\begin{array}{c} x\\ y\\ z \end{array} 
	\right]
	\label{eq:mat_pixel_t_image}
\end{equation}

\begin{equation}
	Z_c \left[ 
	\begin{array}{c} u\\ v\\ 1 \end{array} 
	\right] = 
	\begin{bmatrix}
		\alpha_x & S & C_x \\
		0 & \alpha_y & C_y \\
		0 & 0 & 1
	\end{bmatrix} 
	\begin{bmatrix}
		1 & 0 & 0 & 0 \\
		0 & 1 & 0 & 0 \\
		0 & 0 & 1 & 0
	\end{bmatrix}
	\left[ 
	\begin{array}{c} X_c\\ Y_c\\ Z_c\\ 1 \end{array} 
	\right]
	\label{eq:mat_pixel_t_camera}
\end{equation}

\begin{equation}
	Z_c \left[ 
	\begin{array}{c} u\\ v\\ 1 \end{array} 
	\right] = 
	\left[ 
	\begin{array}{c} K_{3\times3} \end{array} 
	\right] 
	\left[ 
	\begin{array}{c} I_{3\times3} | 0 \end{array} 
	\right]
	\left[ 
	\begin{array}{c} X_c\\ Y_c\\ Z_c\\ 1 \end{array}
	\right]
	\label{eq:_mat_camera_instrinsic}
\end{equation}

In summary, the equation \ref{eq:mat_image_t_camera} maps the world point measured in camera coordinates to image coordinates while equation \ref{eq:mat_pixel_t_image} converts these image coordinates to pixel coordinates. Overall mapping from camera coordinates to the pixel coordinates is depicted in the equation \cref{eq:mat_pixel_t_camera}. K in equation \ref{eq:_mat_camera_instrinsic} is known as the camera intrinsic matrix and depends only on the internals of the camera. In general, a 3D world point \( (X, Y, Z)^T \) may not be known in the camera coordinate system however it can be mapped using 4$\times$4 homogeneous transformation matrix. equation \ref{eq:mat_pixel_t_world} is a mapping from a 3D world point to pixel coordinates. The 4$\times$4 homogeneous transformation matrix is known as an extrinsic matrix and describes the position and orientation of the 3D world point in the camera coordinate system.

\begin{equation}
	Z_c \left[ 
	\begin{array}{c} u\\ v\\ 1 \end{array} 
	\right] = 
	\begin{bmatrix}
		\alpha_x & S & C_x \\
		0 & \alpha_y & C_y \\
		0 & 0 & 1
	\end{bmatrix} 
	\begin{bmatrix}
		r_{11} &r_{12}  &r_{13}  &t_x\\
		r_{21} &r_{22}  &r_{23}  &t_y\\
		r_{31} &r_{32}  &r_{33}  &t_z \\
		0 &0  &0  &1
	\end{bmatrix}
	\left[ 
	\begin{array}{c} X\\ Y\\ Z\\ 1 \end{array} 
	\right]
	\label{eq:mat_pixel_t_world}
\end{equation}

The equation \ref{eq:mat_pixel_t_world} can be compactly written by combining both intrinsic and extrinsic matrix known as the projection matrix \textbf{$P_{3X4}$}. Where P = K[R|t], and $\lambda$ is a arbitrary scaling factor for which equation \ref{eq:mat_projection} is satisfied.

\begin{equation}
	\lambda \left[ 
	\begin{array}{c} u\\ v\\ 1 \end{array} 
	\right] = 
	\begin{bmatrix}
		p_{11} &p_{12}  &p_{13}  &p_{14}\\
		p_{21} &r_{22}  &r_{23}  &p_{24}\\
		p_{31} &p_{32}  &p_{33}  &p_{34} \\
	\end{bmatrix}
	\left[ 
	\begin{array}{c} X\\ Y\\ Z\\ 1 \end{array} 
	\right]
	\label{eq:mat_projection}
\end{equation} 
We extract the camera center from the projection matrix as the camera center C = $-\inv{R}t$ or in other words, $t = -RC$. Therefore the projection matrix can be written as,
\begin{equation}
	\begin{split}
		\matr{P} = \matr{K}\ [\matr{R}\ |\ \vect{t}]\\
		= \matr{K}\matr{R}\ [\matr{I}\ |\ -\vect{C}]\\
		= \matr{M}\ [\matr{I}\ |\ \invMat{M}\ \textbf{$p_4$}]
	\end{split}
	\label{eq:projection_split_up}
\end{equation}
where M = KR, K is a 3x3 upper triangular camera matrix, R is a 3x3 rotation matrix and $p_4$ is the last column of the projection matrix.

\section{Camera calibration}
Often in practice, estimating intrinsic and extrinsic parameters are important. Two approaches will be presented here, one using projection matrix from equation \ref{eq:mat_projection} (2D/3D correspondence) and using homography (2D/2D correspondence).

\subsection{2D/3D correspondence}

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\linewidth]{2D_3D.png}
	\caption{Mapping 3D world points on to a 2D image plane.}
	\label{fig:2D_3D}
\end{figure}

The \cref{fig:2D_3D} depicts a 2D image of a 3D object with known 6 unique points. Recall  the equation \ref{eq:mat_projection} projection matrix P maps the world point to pixel coordinates and the equation is valid for the arbitrary scaling factor $\lambda$. First, the projection matrix P will be estimated using a given set of 3D world points and 2D image points and then it will be decomposed to intrinsic and extrinsic matrices as P = K[R|t]. The first step in estimating projection matrix is to convert the equation \ref{eq:mat_projection} into least square problem of type II (see appendix) Ap = 0 subjected to $ \|p\| = 1 $ and solve for vector $\vect{p}$ which is reshaped version of non-trivial elements of the projection matrix P. 

Given a set of N corresponding 2D/3D points (\textbf{$u_i$},\textbf{$X_i$}), a projection matrix P is needed such that

\begin{equation*}
	\lambda\textbf{$u_i$} = P \textbf{$X_i$}\ ,  where\ i = 1......N
\end{equation*}
since the scaling factor $\lambda$ is unknown and has to be estimated while estimating P. we will make use of DLT (direct linear transformation) to convert the above equation to the form Ap = 0.

\begin{equation*}
	\left[ 
	\begin{array}{c} \lambda u_i\\ \lambda v_i\\ \lambda  \end{array} 
	\right] = 
	\begin{bmatrix}
		p_{11} &p_{12}  &p_{13}  &p_{14}\\
		p_{21} &r_{22}  &r_{23}  &p_{24}\\
		p_{31} &p_{32}  &p_{33}  &p_{34} \\
	\end{bmatrix}
	\left[ 
	\begin{array}{c} X_i\\ Y_i\\ Z_i\\ 1 \end{array} 
	\right]
\end{equation*} 

extracting the bottom row, equation for $\lambda$ and substituting $\lambda$ in other 2 rows yields
\begin{equation*}
	\lambda = p_{31}\textbf{$X_i$} + p_{32}\textbf{$Y_i$} + p_{33}\textbf{$Z_i$} + p_{34}
\end{equation*}

\begin{equation*}
	\begin{split}
		\textbf{$u_i$}\lambda = \textbf{$u_i$} (p_{31}\textbf{$X_i$} + p_{32}\textbf{$Y_i$} +  p_{33}\textbf{$Z_i$} + p_{34}) = p_{11}\textbf{$X_i$} + p_{12}\textbf{$Y_i$} + p_{13}\textbf{$Z_i$} + p_{14}\\
		\textbf{$v_i$}\lambda = \textbf{$v_i$} (p_{31}\textbf{$X_i$} + p_{32}\textbf{$Y_i$} +  p_{33}\textbf{$Z_i$} + p_{34}) = p_{21}\textbf{$X_i$} + p_{22}\textbf{$Y_i$} + p_{23}\textbf{$Z_i$} + p_{24}
	\end{split}
\end{equation*}
Rearrangement of the above equations leads to a linear system of equations with non-trivial elements of the projection matrix being reshaped into a vector $\vect{p}$.   

\begin{equation}
	\begin{bmatrix}
		\textbf{$X_i$} & \textbf{$Y_i$} & \textbf{$Z_i$} & 1 & 0 & 0 & 0 & 0 & \textbf{$-u_i X_i$} & \textbf{$-u_iY_i$} & \textbf{$-u_i Z_i$} & -u_i\\
		0 & 0 & 0 & 0 & \textbf{$X_i$} & \textbf{$Y_i$}  & \textbf{$Z_i$} & 1 & \textbf{$-v_iX_i$} & \textbf{$-v_iY_i$} & \textbf{$-v_iZ_i$} & -v_i
	\end{bmatrix}
	\textbf{p} = \textbf{0}
	\label{eq:mat_projection_1}
\end{equation} 

\begin{equation*}
	with\ \textbf{p} = \left (p_{11}, p_{12}.........p_{33},p_{34})^T  \in \mathcal{R}^{12} \right. 
\end{equation*}
A projection matrix has 12 non-trivial elements thus 11 degrees of freedom (ignoring scaling) therefore it is necessary to have 11 equations to solve for P. Each pair of 2D/3D point correspondences leads to 2 equations thus a minimum of 6 2D/3D point correspondences are required. Stacking these 6 equations along rows leads to a linear system of equations Ap=0 as shown in \cref{eq:mat_projection_final}. This process of rearranging the equations is known as direct liner transformation (DLT). 
\begin{equation}
	\begin{bmatrix}
		\textbf{$X_1$} & \textbf{$Y_1$} & \textbf{$Z_1$} & 1 & 0 & 0 & 0 & 0 & \textbf{$-u_1 X_1$} & \textbf{$-u_1Y_1$} & \textbf{$-u_1 Z_1$} & -u_1\\
		0 & 0 & 0 & 0 & \textbf{$X_1$} & \textbf{$Y_1$}  & \textbf{$Z_1$} & 1 & \textbf{$-v_1X_1$} & \textbf{$-v_1Y_1$} & \textbf{$-v_1Z_1$} & -v_1\\
		\vdots & \vdots  & \vdots  & \vdots  & \vdots  & \vdots  & \vdots  & \vdots  & \vdots  & \vdots  & \vdots  & \vdots\\
		\textbf{$X_6$} & \textbf{$Y_6$} & \textbf{$Z_6$} & 1 & 0 & 0 & 0 & 0 & \textbf{$-u_6 X_6$} & \textbf{$-u_6Y_6$} & \textbf{$-u_6 Z_6$} & -u_6\\
		0 & 0 & 0 & 0 & \textbf{$X_6$} & \textbf{$Y_6$}  & \textbf{$Z_6$} & 1 & \textbf{$-v_6X_i$} & \textbf{$-v_6Y_6$} & \textbf{$-v_6Z_6$} & -v_6\\
	\end{bmatrix}
	\textbf{p} = \textbf{0}
	\label{eq:mat_projection_final}
\end{equation} 

\textbf{Exact solution.} With a minimum of 6 correspondences, the solution to \ref{eq:mat_projection_final} will be exact which means 3D world points will be exactly gets projected to their measured image points correspondingly. The exact solution p, to Ap = 0 is the right null-space of matrix A.\\

\textbf{Over-determined solution.} Practical measurements will be noisy due to various reasons and therefore we may require more than 6 2D/3D correspondences. In this case, the solution to Ap = 0 will be obtained by minimizing the algebraic or geometric error of projection, subjected to some valid constraints.\\ 

\textbf{Minimizing algebraic error.} In this case the approach is,   

\begin{equation}
	\begin{matrix}
		\displaystyle \min & \|Ap\| & \textrm{s.t.} & \|p\| = 1\\
	\end{matrix}
	\label{eq:min_algebraic}
\end{equation}
The solution to \cref{eq:min_algebraic} is obtained from unit singular value of A corresponding to the smallest singular value (the least square problem of type II, see appendix).\\

\textbf{Minimizing geometric error.} First let us define what is geometric error. Let us recall \cref{eq:mat_projection}, \textbf{$u_i$} = P\textbf{$X_i$}, suppose we know 3D world points \textbf{$X_i$} far more accurately than the measured image points then the geometric error in the image is 
\begin{equation*}
	\begin{matrix}
		\displaystyle \sum_{i=1}^{n} d (u_i, \hat{u}_i)^2 
	\end{matrix}
\end{equation*}
where $u_i$ is measured point in the image and $\hat{u}_i$ is P \textbf{$X_i$} which is the exact projection of \textbf{$X_i$} on to the image under P. If the measurement errors are Gaussian then the solution to 
\begin{equation}
	\begin{matrix}
		\displaystyle \min_{P} \sum_{i=1}^{n} d (u_i, P\textbf{$X_i$})^2 
	\end{matrix}
	\label{eq:min_geometric}
\end{equation}

is the maximum likelihood of P  as per \cite{Hartley:2003:MVG:861369}. Minimizing geometric error requires non-linear iterative methods such as Levenberg-Marquardt (LM) algorithms. Local minima can be found via LM, in order to find the global minima, the initial starting point can be linear solution obtained from \cref{eq:min_algebraic}.\\

Having estimated projection matrix task at the hand is to decompose $P = K[R\ |\ t] = M[I\ |\ \inv{M}\ {p_4}]$ where $M = KR$. Decomposing M to K and R can be achieved using RQ decomposition with the constraint that diagonal entries of the K matrix has to be positive.

\subsection{2D/2D correspondence}
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\linewidth]{2D_2D.png}
	\caption{Mapping 2D planar world points (Z = 0) on to a 2D image plane.}
	\label{fig:2D_2D}
\end{figure}

The disadvantage of 2D/3D correspondence way of estimating the projection matrix is that complete knowledge of the 3D location has to be known and it has to be precise as well. There is a flexible and computationally inexpensive method to estimate the projection matrix developed by Zhang \cite{Zhang} using a 2D planar object in 3D space as shown in \cref{fig:2D_2D} along with its 2D image. Let us recall the equation \ref{eq:mat_pixel_t_world} 

\begin{equation*}
	Z_c \left[ 
	\begin{array}{c} u\\ v\\ 1 \end{array} 
	\right] = 
	\begin{bmatrix}
		\alpha_x & S & C_x \\
		0 & \alpha_y & C_y \\
		0 & 0 & 1
	\end{bmatrix} 
	\begin{bmatrix}
		r_{11} &r_{12}  &r_{13}  &t_x\\
		r_{21} &r_{22}  &r_{23}  &t_y\\
		r_{31} &r_{32}  &r_{33}  &t_z \\
		0 &0  &0  &1
	\end{bmatrix}
	\left[ 
	\begin{array}{c} X\\ Y\\ Z\\ 1 \end{array} 
	\right]
\end{equation*}

Using 2D planar object we eliminate Z coordinate thereby eliminating $3^{rd}$ column of the extrinsic matrix.

\begin{equation*}
	Z_c \left[ 
	\begin{array}{c} u\\ v\\ 1 \end{array} 
	\right] = 
	\begin{bmatrix}
		\alpha_x & S & C_x \\
		0 & \alpha_y & C_y \\
		0 & 0 & 1
	\end{bmatrix} 
	\begin{bmatrix}
		r_{11} &r_{12}  &r_{13}  &t_x\\
		r_{21} &r_{22}  &r_{23}  &t_y\\
		r_{31} &r_{32}  &r_{33}  &t_z \\
		0 &0  &0  &1
	\end{bmatrix}
	\left[ 
	\begin{array}{c} X\\ Y\\ 0\\ 1 \end{array} 
	\right]
\end{equation*}

\begin{equation}
	Z_c \left[ 
	\begin{array}{c} u\\ v\\ 1 \end{array} 
	\right] = 
	\begin{bmatrix}
		\alpha_x & S & C_x \\
		0 & \alpha_y & C_y \\
		0 & 0 & 1
	\end{bmatrix} 
	\begin{bmatrix}
		r_{11} &r_{12}  &t_x\\
		r_{21} &r_{22}  &t_y\\
		r_{31} &r_{32}  &t_z \\
		0 &0 &1
	\end{bmatrix}
	\left[ 
	\begin{array}{c} X\\ Y\\ 1 \end{array} 
	\right]
	\label{eq:mat_2D_pixel_t_world}
\end{equation}

As in the case of 2D/3D problem setup, the equation \ref{eq:mat_2D_pixel_t_world} can be compactly written by combining both intrinsic and extrinsic matrices known an Homography matrix \textbf{$H_{3\times3}$}. And $\lambda$ is a arbitrary scaling factor for which equation \ref{eq:mat_homography} is satisfied. In other words, a projection matrix \textbf{$P_{3\times4}$} in planar case reduces to homography matrix \textbf{$H_{3\times3}$}.

\begin{equation}
	\lambda \left[ 
	\begin{array}{c} u\\ v\\ 1 \end{array} 
	\right] = 
	\begin{bmatrix}
		h_{11} &h_{12}  &h_{13} \\
		h_{21} &h_{22}  &h_{23} \\
		h_{31} &h_{32}  &h_{33} \\
	\end{bmatrix}
	\left[ 
	\begin{array}{c} X\\ Y\\ 1 \end{array} 
	\right]
	\label{eq:mat_homography}
\end{equation} 

As in the case of 2D/3D problem, the homography matrix H will be estimated using a given set of 2D planar world points and 2D image points and then it will be decomposed to intrinsic and extrinsic matrices.\\

Given a set of N corresponding 2D/2D points (\textbf{$u_i$},\textbf{$X_i$}), a homography matrix H is needed such that
\begin{equation*}
	\lambda\textbf{$u_i$} = H \textbf{$X_i$}\ ,  where\ i = 1......N
\end{equation*}
Problem set up in 2D/2D case is very similar to 2D/3D problem except that the homography matrix has 9 non-trivial elements thus 8 degrees of freedom (ignoring scaling) therefore 8 independent equations are necessary to estimate H, hence 4 2D/2D point correspondences are required. At this point, the same estimating procedure used previously can be employed.\\

\textbf{Exact solution.} With a minimum of 4 correspondences, the solution to $Ah = 0 $ will be exact which means 2D world points will be exactly gets projected to their measured image points correspondingly. The exact solution h, to Ah = 0 is the right null space of matrix A.\\

\textbf{Over-determined solution.} In this case solution to Ah = 0 will be obtained by minimizing the algebraic or geometric error of projection, subjected to some valid constraints.\\ 

\textbf{Minimizing algebraic error.} In this case the approach is,   

\begin{equation}
	\begin{matrix}
		\displaystyle \min & ||Ah|| & \textrm{s.t.} & ||h|| = 1\\
	\end{matrix}
	\label{eq:min_2D_algebraic}
\end{equation}
The solution to \cref{eq:min_2D_algebraic} is obtained from unit singular value of A corresponding to the smallest singular value (the least square problem of type II, see appendix).\\

\textbf{Minimizing geometric error} The geometric error in the image in 2D/2D case is
\begin{equation*}
	\begin{matrix}
		\displaystyle \sum_{i=1}^{n} d (u_i, \hat{u}_i)^2 
	\end{matrix}
\end{equation*}
where $u_i$ is measured point in image and $\hat{u}_i$ is H \textbf{$X_i$} which is the exact projection of \textbf{$X_i$} on to the image under H. If the measurement errors are Gaussian then the solution to 
\begin{equation}
	\begin{matrix}
		\displaystyle \min_{P} \sum_{i=1}^{n} d (u_i, H\textbf{$X_i$})^2 
	\end{matrix}
	\label{eq:min_2D_geometric}
\end{equation}
is the maximum likelihood of H. Minimizing geometric error requires non-linear iterative methods such as Levenberg-Marquardt (LM) algorithms. Local minima can be found via LM, in order to find the global minima, initial starting point can be linear solution obtained from \ref{eq:min_2D_algebraic}.\\

The disadvantage of 2D/2D homography is that with a single homography, not all the parameters of a projection matrix can be determined as Z coordinate of the world point is eliminated.  
\begin{equation*}
	\textbf{H} = \begin{bmatrix}
		\textbf{p}_{1} & \textbf{p}_{2}  & \textbf{p}_{4} \\
	\end{bmatrix}
\end{equation*} 
The decomposition of H to obtain extrinsic and intrinsic matrices is not possible, therefore, follows a different approach. We will first see how to get the intrinsic matrix K and as soon we have K, obtaining extrinsic is trivial.\\

\textbf{Intrinsics} Recall that $H = K\ [\textbf{r}_1\  \textbf{r}_2\  \textbf{t}] = [\textbf{p}_1\ \textbf{p}_2\ \textbf{p}_3]$ and the fact that R is a rotational matrix and can be expressed as,

\begin{equation*}
	\begin{split}
		\mathbf{r}_1^\intercal \mathbf{r}_2  = 0\\ 
		\mathbf{r}_1^\intercal \mathbf{r}_1  = \mathbf{r}_2^\intercal \mathbf{r}_2  = 1 
	\end{split}	
\end{equation*}
rewriting with K gives,
\begin{equation*}
	\begin{split}
		\mathbf{p}_1^T K^{-T} K^{-1} \mathbf{p}_2 = 0 \\
		\mathbf{p}_1^T K^{-T} K^{-1} \mathbf{p}_1 = \mathbf{p}_2^T K^{-T} K^{-1} \mathbf{p}_2   = 1
	\end{split}	
\end{equation*}
defining $\textbf{$\omega$}$ = $K^{-T} K^{-1}$ as a symmetric matrix, above equations can be written as,
\begin{equation*}
	\omega = 
	\begin{bmatrix}
		\omega_{11} &\omega_{12}  &\omega_{13} \\
		\omega_{12} &\omega_{22}  &\omega_{23} \\
		\omega_{13} &\omega_{23}  &\omega_{33} \\
	\end{bmatrix}
\end{equation*} 
\begin{equation}
	\begin{split}
		\mathbf{p}_1^T \omega \mathbf{p}_2 = 0 \\
		\mathbf{p}_1^T \omega \mathbf{p}_1 - \mathbf{p}_2^T \omega \mathbf{p}_2  = 0
	\end{split}	
	\label{eq:intrinsic}
\end{equation}

$\textbf{p}_1$ and $\textbf{p}_2$ are known from homography matrix and $\vect{\omega}$ has to be calculated. $\vect{\omega}$  can be estimated with techniques that we employed earlier with DLT () and SVD \cite{soderkvist2009using} by defining $ \textbf{b} = (\omega_{11}\ \omega_{12}\ \omega_{13}\  \omega_{22}\ \omega_{23}\ \omega_{33})^T $ and solving $ \textbf{A} \textbf{b} = 0.$ Each homography provides 2 independent rows therefore 3 such homographies are required to estimate $\vect{\omega}$ . Once the $\vect{\omega}$  is computed it can be decomposed in to $K^{-T} K^{-1}$ using Cholesky decomposition.\\

\textbf{Extrinsics.} Computing extrinsics post intrinsics is very straight forward as given by \cite{Zhang} we will first calculate rotation matrix followed by translation vector. $H = K\ [\textbf{r}_1\  \textbf{r}_2\  \textbf{t}] $ having known K we can rearrange as $\inv{K}\ H = H^{'} =  [\textbf{h}_1^{'}\  \textbf{h}_2^{'}\  \textbf{h}_3^{'}] $ which is equal to $[\textbf{r}_1\  \textbf{r}_2\  \textbf{t}] $. Since the rotational matrix is a orthogonal matrix, the third column is orthogonal to first 2 and therefore can be calculated by vector cross product $ [\textbf{h}_1^{'} \times \textbf{h}_2^{'} ] $. Hence, the complete rotational matrix is  $ Q = [\textbf{h}_1^{'}\ \textbf{h}_2^{'}\ \textbf{h}_1^{'} \times\ \textbf{h}_2^{'} ] $ again which is equal to $ R = [\textbf{r}_1\ \textbf{r}_2\ \textbf{r}_1\ \times\ \textbf{r}_2\ ] $ which may not be true due to the noisy data. Therefore,  it is necessary to find a best 3x3 rotation matrix  R from a given 3x3 matrix Q. The "best" in the sense of smallest Frobenius norm of the difference $[R-Q]$ (see appendix). If the SVD of given matrix is $ Q = UDV^T $, then the best rotation matrix is given by $UV^T$. The translation vector can be calculated as  $ \textbf{t} = \textbf{h}_3^{'}$ but the vector $\textbf{t} $ has to be normalized therefore $\textbf{t} = \textbf{h}_3^{'} / \|\textbf{h}_1^{'}\|$.

\subsection{Lens distortions} A world point is mapped to image plane as \( (x, y, z)^T \) = \( (\textit{f}\frac{X_c}{Z_c}, \textit{f}\frac{Y_c}{Z_c}, \textit{f} )^T\) then image points are mapped to pixel coordinates as \( (u, v, z)^T \) = \( ( x+C_x, y+C_y, \textit{f} )^T \) however, usually, there are radial and tangential distortions in the lens as depicted in \cref{fig:distortion} therefore, image coordinates have to be corrected before mapping to pixel coordinates. The corrected image coordinates are

\begin{equation}
	\begin{split}
		x^{'} = x \times \frac{1+k_1r^2+k_2r^4+k_3r^6}{1+k_4r^2+k_5r^4+k_6r^6} + 2p_1xy + p_2(r^2+2x^2)\\
		y^{'} = y \times \frac{1+k_1r^2+k_2r^4+k_3r^6}{1+k_4r^2+k_5r^4+k_6r^6} + p_1(r^2+2y^2) + 2p_2xy\\ 
	\end{split}	
	\label{eq:lens_distortion}
\end{equation}

\noindent where $ r^2 = (x^2+y^2) $ , $ k_1 .....k_6 $ are the radial distortion coefficients and  $ p_1, p_2 $ are the tangential distortion coefficients. It is these corrected image coordinates are mapped to pixel coordinates as \( (u, v, z)^T \) = \( ( x^{'} +C_x, y^{'} +C_y, \textit{f} )^T \).

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\linewidth]{distortion.png}
	\caption{Barrel distortion $(typically \ k_1 > 0 )$ and pincushion distortion $(typically k_1 < 0)$ \cite{OpenCV}} 
	\label{fig:distortion}
\end{figure}
\newpage
Camera calibration is the process of determining, intrinsic, extrinsic parameters along with distortion coefficients. The \cref{tab:Calibration_summary_table} summarizes the calibration process discussed above.

\begin{table}[hbt!]
	\centering
	\begin{tabular}{|cccccc|}
		\hline
		correspondences & Min. points & No.of images & [R\ |\ t] & K & world points\\ 
		\hline
		2D im./3D world & $\geq$ 6 & 1 & \checkmark & \checkmark & given \\
		2D im./2D world & $\geq$ 4 & 3 & \checkmark & \checkmark  & given \\
		2D im./2D world & $\geq$ 4 & 1 & \checkmark  & given & given \\
		\hline
	\end{tabular}
	\caption{Calibration summary table}
	\label{tab:Calibration_summary_table}
\end{table}


\section{Pixel to 3D points} 
Having obtained RGB and corresponding depth images of the scene (assuming depth to RGB calibrated), any specific pixel coordinates can be out-projected to obtain 3D locations expressed in the camera coordinates. Recall \cref{eq:mat_pixel_t_camera}
a 3D world point \((X_c, Y_c, Z_C)^T\) is mapped to the pixel coordinates and can be expressed as matrix multiplication. $\lambda$ is the scaling factor.

\begin{equation*}
\lambda \left[ 
\begin{array}{c} u\\ v\\ 1 \end{array} 
\right] = 
\begin{bmatrix}
\alpha_x & S & C_x \\
0 & \alpha_y & C_y \\
0 & 0 & 1
\end{bmatrix} 
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{bmatrix}
\left[ 
\begin{array}{c} X_c\\ Y_c\\ Z_c\\ 1 \end{array} 
\right]
\label{eq:mat_pixel_t_camera_p}
\end{equation*}

By rearranging the above equation \((X_c, Y_c, Z_C)^T\) can be expressed as below.

\begin{equation}
\begin{split}
X_c &= \frac{u - C_x}{\alpha_x} * \lambda \\
Y_c &= \frac{v - C_y}{\alpha_y} * \lambda \\
Z_c &= \lambda 
\end{split}
\label{eq:out_projection}
\end{equation}

\section{Hand-eye calibration}

In general, robotic systems make use of many sensors and often require knowledge of static transformation between them. The \cref{fig:handeye_calibration_setup} depicts one such situation. These transformations can be estimated using a process known as hand-eye calibration  \cite{ernst}.

\begin{figure}[hbt!]
	\centering
	\includegraphics[scale=0.5]{handeye_calibration.png}
	\caption{Generic hand-eye calibration setup \cite{Handeyec96:online} (Modified). setup consists of a robot, a chessboard mounted to robot end-effector, and a RGB-D camera} 
	\label{fig:handeye_calibration_setup}
\end{figure}

There are four transformations, namely end-effector  to base  $\tfMat{Base}{T}{EE}$, chessboard to end-effector  $\tfMat{EE}{T}{CB}$, chessboard to camera  $\tfMat{Cam}{T}{CB}$ and camera to base  $\tfMat{Base}{T}{Cam}$. Defining new variables  $\tfMat{Base}{T}{EE}$ as M,  $\tfMat{EE}{T}{CB}$ as X,  $\tfMat{Base}{T}{Cam}$ as Y and  $\tfMat{Cam}{T}{CB}$ as N we can write the transformation equation as 

\begin{equation}
	\matr{M}_i\matr{X} - \matr{Y}\matr{N}_i = 0 
	\label{eq:hand-eye}
\end{equation}

where $\matr{M}$ is obtained through forward kinematics, $\matr{N}$ is obtained by calculating the pose of the calibration pattern from the camera image. Hand-eye calibration aims to solve the two unknowns in \cref{eq:hand-eye} i.e the chessboard position with respect to the robot's end-effector $\matr{X}$ and the camera position with respect to the robot base $\matr{Y}$.

The equation \ref{eq:hand-eye} is transformed to a system of linear equations as  

\begin{equation}
	\centering
	\begin{split}
		\matr{A} \vect{w} = \vect{b}\\
	\end{split}
	\label{eq:hand-eye_least_square}
\end{equation}

where, $\vect{w} = [x_{11}, x_{21},.....,x_{34},y_{11},y_{21},..........,y_{34}]$ is a vector of non-trivial elements of matrices $\matr{X}$ and $\matr{Y}$. 

\begin{equation*}
	\matr{A} =
	\left[ 
	\begin{array}{c} A_1\\ A_2\\ .\\ .\\ .\\ A_n \end{array} 
	\right]
	\vect{b} = 
	\left[ 
	\begin{array}{c} b_1\\ b_2\\ .\\ .\\ .\\ b_n \end{array} 
	\right]
\end{equation*}

\noindent where, $\matr{A}$ $\in$ $\mathbb{R}^{12n\times24}$, $\vect{b}$ $\in$ $\mathbb{R}^{12n}$ and defined as,

\begin{equation*}
	\matr{A}_i =
	\left[ 
	\begin{array}{cc} 
		\begin{array}{cccc} 
			\mathtt{R}[\matr{M}]_i(\matr{N}_i)_{11} & \mathtt{R}[\matr{M}]_i(\matr{N}_i)_{21}  &\mathtt{R}[\matr{M}]_i(\matr{N}_i)_{31}  & \matr{Z}_{3\times3}\\ 
			\mathtt{R}[\matr{M}]_i(\matr{N}_i)_{12} & \mathtt{R}[\matr{M}]_i(\matr{N}_i)_{22}   &\mathtt{R}[\matr{M}]_i(\matr{N}_i)_{32}  & \matr{Z}_{3\times3}\\
			\mathtt{R}[\matr{M}]_i(\matr{N}_i)_{13} & \mathtt{R}[\matr{M}]_i(\matr{N}_i)_{23}   &\mathtt{R}[\matr{M}]_i(\matr{N}_i)_{33}  & \matr{Z}_{3\times3}\\
			\mathtt{R}[\matr{M}]_i(\matr{N}_i)_{14} & \mathtt{R}[\matr{M}]_i(\matr{N}_i)_{24}   &\mathtt{R}[\matr{M}]_i(\matr{N}_i)_{34}  & \matr{Z}_{3\times3}
		\end{array} &  -\matr{E}_{12}
	\end{array}
	\right]
\end{equation*}

\noindent and

\begin{equation*}
	\vect{b}_i =
	\left[ 
	\begin{array}{c} 
		\matr{Z}_{9\times1} \\
		-\mathtt{T}[\matr{M}_i]
	\end{array}
	\right]
\end{equation*}

\noindent Here, $ \mathtt{R}[\matr{M}]_i \in \mathbb{R}^{3\times3} $ is rotational part of $ \matr{M}_i $ and $ \mathtt{T}[\matr{M}_i] \in \mathbb{R}^{3} $ is translational part of $ \matr{M}_i $. $ \matr{Z}_{m \times n} $ is a m $\times$ n zero matrix and $\matr{E}_{k} $ is a k $ \times $ k identity matrix.
\\
\noindent \textbf{Over-determined solution} The system of equation \ref{eq:hand-eye} is a form of least square of type I (see appendix) and can be solved using QR factorization \cite{QRdecomposition} by minimizing quadratic error.

\begin{equation*}
	\sum_{i=1}^n \|\matr{M}_i\matr{X} - \matr{Y}\matr{N}_i\|_{F}\
\end{equation*}

\noindent Where, $\|.\|_{f}$ is Frobenoius norm. The above algorithm is called QR-24 according to \cite{ernst}.\\


\noindent \textbf{Orthonormalization} Note that matrices computed not necessarily orthogonal therefore we need to orthonormalize using singular value decomposition (SVD)\\


\noindent \textbf{Calibration error} Having finished the hand-eye calibration, It is necessary to investigate the accuracy of the obtained results. Accuracy can be obtained by rearranging the equation \ref{eq:hand-eye} as,

\begin{equation*}
	(\tfMat{EE}{T}{CB})^{-1} (\tfMat{Base}{T}{EE})^{-1} \tfMat{Base}{T}{Cam} \tfMat{Cam}{T}{CB} = \matr{I}_{4\times4}
\end{equation*}

The above equation will not hold in reality due to noisy data, camera calibrations error etc. RHS of the above equation will be fully populated matrix instead of an identity matrix. two error metrics can be defined to individually evaluate the rotation and translation parts per \cite{ernst}. 

\begin{equation*}
	\centering
	\begin{split}
		Let\ \matr{A}_{4\times4}^{'} = (\tfMat{EE}{T}{CB})^{-1} (\tfMat{Base}{T}{EE})^{-1} \tfMat{Base}{T}{Cam} \tfMat{Cam}{T}{CB} \\
		\matr{S}\matr{V}\matr{D}\ (\matr{A}_{4\times4}^{'}) = \matr{U}\matr{D}\matr{V}\  then,\\ 
		\matr{A}_{4\times4} = \matr{U}\matr{V}^T
	\end{split}
\end{equation*} 

\noindent then, translation error can be defined as,

\begin{equation*}
	\vect{e}_{trans}[\matr{A}] = \sqrt{\matr{A}_{4\times1}^{2} + \matr{A}_{4\times2}^{2} + \matr{A}_{4\times3}^{2}}
\end{equation*} 

\noindent let (k,$\theta$) be axis angle representation \cite{axis-angle} of $\mathtt{R}[\matr{A}]$ then the rotational error metric is,

\begin{equation*}
	\vect{e}_{rot}[\matr{A}] = |\theta|
\end{equation*} 

\noindent in the rotation metric the axis of rotation is neglected.

\section{Point cloud registration}
The objective of any point cloud registration algorithm is to find the best transformation that aligns 2 point clouds (source and target). A common point cloud registration technique used is the iterative closest point (ICP) algorithm \cite{besl1992method}. A brief introduction to ICP is provided in this section. 

Let \( \displaystyle P = \{p_1, p_2,...,p_m\} \) be the source point cloud set that needs to be aligned with the target point cloud set  \( \displaystyle Q = \{q_1, q_2,...,q_n\} \)  with correspondences  \( \displaystyle C = \{ (i,j) \} \).

objective : find the best transformation \( \displaystyle T = [R|t] \) that aligns the source point cloud set to the target point cloud set which minimizes the mean square objective function given below.

\begin{equation}
\begin{matrix}
\displaystyle  \matr{E(\matr{R}|\vect{t})} = \sum_{(i,j) \in C} \| \vect{q}_i - \matr{R}\vect{p}_j -  \vect{t}\|^2 \\
or \\
\displaystyle  \matr{E(\matr{R}|\vect{t})} = \sum_{(i,j) \in C} \| \vect{q}_i - \matr{T[R|t]}\vect{p}_j\|^2 \\
\end{matrix}
\label{eq:mean_square_objective_function}
\end{equation}

where, R is a 3$\times$3 rotation matrix, t is a 3$\times$1 translation vector, and T[R|t] is a 4$\times$4 homogeneous transformation matrix.

\textbf{Case 1: Known data association.} In this case each point in source point cloud set \( \displaystyle p_i \) corresponds to \( \displaystyle q_j \) point in the target point cloud set with same index or to any known index. Then, \cref{eq:mean_square_objective_function} can be solved by finding the center of mass and singular value decomposition.

The center of mass for the source point cloud set and target point cloud set is 

\begin{equation}
\begin{matrix}
\displaystyle  \mu_p = \frac{1}{\|C\|} \sum_{(i,j) \in C} p_i \\
\displaystyle  \mu_q = \frac{1}{\|C\|} \sum_{(i,j) \in C} q_i 
\end{matrix}
\label{eq:center_of_mass_clouds}
\end{equation} 
 
By subtracting the center of mass from every point in the source and target point cloud, we get

\begin{equation}
\begin{matrix}
\displaystyle  {Q'} =  \{q_i- \mu_q \} = \{ {q_i'} \}\\
\displaystyle  {P'} = \{p_j - \mu_p \} = \{ {p_j'} \} 
\end{matrix}
\label{eq:subtract_center_of_mass}
\end{equation} 

minimizing \cref{eq:mean_square_objective_function} is equivalent to minimizing,

\begin{equation}
\begin{matrix}
\displaystyle  \matr{E'(R)} = \sum_{(i,j) \in C} \| [q_1',q_2',...., q_n']  - R [p_1',p_2',...., p_m']\|_F^2 
\end{matrix}
\label{eq:orthogonal_procrustes_problem}
\end{equation}

and is called orthogonal procrustes problem and can be solved by constructing cross-covariance matrix of 2 point cloud sets and SVD \cite{soderkvist2009using}. The cross-covariance matrix of 2 point cloud sets is,

\begin{equation}
\begin{split}
W = \sum_{(i,j) \in C} \{q_i- \mu_q \} \{p_i - \mu_p \}^T \\
W = \sum_{(i,j) \in C} {q_i'} {p_j'}^T 
\end{split}
\label{eq:cross-covariance_matrix}
\end{equation} 

Use the SVD to decompose the cross-covariance matrix hence find the transformation between  2 point clouds.

\begin{equation}
\begin{split}
W &= UDV^T \ then, \\
R &= UV^T \\
t &= \mu_q - R\mu_p \\
\end{split}
\label{eq:solution_orthogonal_procrustes_problem}
\end{equation} 

In conclusion, a point cloud matching algorithm estimates the 3$\times$3 rotation matrix R and 3$\times$1 translation vector t. 

\begin{equation*}
\begin{split}
(T[R|t], MSE) = PointCloudRegistration(P,Q) \\
\end{split}
\end{equation*}
where, T[R|t] is the 4$\times$4 homogeneous transformation between source and target point cloud and MSE is the mean square error of matching as in \cref{eq:mean_square_objective_function}.\\


\textbf{Case 2: Unknown data association.} When the correspondence between source \& target point cloud set is unknown, we resort to iterative closest point algorithm. Where for each point in the source point cloud set \( \displaystyle P = \{p_1, p_2,...,p_m\} \) corresponding closest point in the target point cloud \( \displaystyle Q = \{q_1, q_2,...,q_n\} \) is calculated before proceeding to the point cloud matching. 

The Euclidean distance $d(\vec{p}_1 , \vec{p}_2)$ between two points $\vec{p}_1 = (x_1, y_1, z_1)$ and $\vec{p}_2 = (x_2, y_2, z_2) $ is 
\begin{equation*}
	\begin{split}
		d(\vec{p}_1 , \vec{p}_2) = \|  \vec{p}_1  -  \vec{p}_2 \|  = \sqrt{ {(x_2-x_1)^2} + {(y_2-y_1)^2} + { (z_2-z_1)^2} }\\
	\end{split}
\end{equation*}

The distance between a point $p$ in the source point cloud set and the target point cloud set \( \displaystyle Q = \{q_1, q_2,...,q_n\} \), $ d(\vec{p} , Q) $ is

\begin{equation}
	\begin{split}
		d(\vec{p} , Q) = \min_{i \in Q}  \ d(\vec{p} , \vec{q}_i)\\
		\\
		d(\vec{p} , Q) = \min_{i \in Q}  \|  \vec{p}  -  \vec{q}_i \|
	\end{split}
	\label{eq:min_point_cloud}
\end{equation}

The point in target point cloud $q* \in Q$ that yields a minimum distance is the closest point. The unknown data association is solved when closest point computation is performed for each point in the source point cloud set \( \displaystyle P = \{p_1, p_2,...,p_m\} \).

\begin{equation}
\begin{split}
C = ComputeClosestPoint(P,Q)
\end{split}
\label{eq:compute_closest_point}
\end{equation}

where \( \displaystyle C = \{ (i,j) \} \) is the computed correspondence set for each point in the source point cloud. Having computed the correspondence set, one can proceed with point cloud registration. Each point the source point cloud is updated via $ P_{k+1} = T[R|t] P_k $ 

\begin{equation}
\begin{split}
P_{k+1} = ApplyRegistration(T[R|t],P_k)
\end{split}
\label{eq:apply_registration}
\end{equation}

where, $P_{k+1}$ are the points after applying the registration (homogeneous representation is used here to enable matrix multiplication).

\subsection{Iterative closest point algorithm}

\begin{enumerate}
	\item Given \( \displaystyle P = \{p_1, p_2,...,p_m\} \) be the source point cloud set that needs to be aligned with the target point cloud set  \( \displaystyle Q = \{q_1, q_2,...,q_n\} \) with unknown data association.
	\item Initialize the iteration $P_0 = P$ , $T[R|t]$ = 4X4 identity [], $k = 0$
	\begin{enumerate}
		\item $C_k = ComputeClosestPoint(P_k,Q)$
		\item $(T[R|t]_k, MSE_k) = PointCloudRegistration(P_0, C_k)$
		\item $P_{k+1} = ApplyRegistration(T[R|t]_k, P_0)$
		\item Terminate the iteration when 
		\begin{enumerate}
			\item change in the mean square error is less than a predefined threshold $\tau > 0$ i.e. $ MSE_k - MSE_{k+1} < \tau $.
			\item or when iteration exceeds the predefined number.
		\end{enumerate} 
	\end{enumerate}
\end{enumerate}

\section{Simultaneous Localization and Mapping}

\subsection{Map}
Formally, a map \textit{m} is a list of objects in the environment along with their properties \cite{10.5555/1121596}. Especially, in this case, the map is a collection of electrodes and their locations on the EEG cap. 

\begin{equation*}
	\begin{split}
		m = \{m_1, m_2,....m_N \} \\
	\end{split}
	\label{eq:map_definition}
\end{equation*} 

where, N is the total number of electrodes on the cap and $m_n$  where $1 \le n \le N$ represents the 3D location of the $n^{th}$ electrode. 

\subsection{Localization}
In simple words, localization is the pose estimation problem. It is the process of determining the pose of a camera relative to the given map of the electrodes using a wide variety of sensor data i.e RGB-D images and odometry etc. Localization is also the problem of establishing the correspondence between the coordinate system based on which the map is defined (often, global coordinate system) and the camera's local coordinate system. For the problem addressed in this thesis, this correspondence enables the camera to express the electrodes in the map in its local coordinate system. Unfortunately, sensor measurements are noisy, therefore pose has to be inferred from the noisy data which makes localization a difficult problem to solve. There are many probabilistic approaches to solve localization problems, the reader is encouraged to refer \cite{10.5555/1121596} for more details.
\\

\subsection{SLAM} 
Simultaneous localization and mapping (SLAM) is a special case, for the problem at hand, where the camera neither has the pre-build map of the electrodes nor does it know its pose relative to the map. The camera can however be moved along a trajectory either with a help of a robot or manually within the environment and also collect RGB and depth images along the way. Finally, the task is to build the map of the electrodes and simultaneously localize the camera with this map. Since measurements are prone to noise, SLAM problems are modeled probabilistically.
\\

\textbf{Probabilistic formulation of SLAM.} The camera moves with in an unknown environment along a trajectory. Camera poses along the trajectory are described by the random variables $x_{1:t} = \{x_1,......, x_t\}$. A set of odometry can be generated along the trajectory $u_{1:t} = \{u_1,......, u_t\}$ and perceived 3D electrode position can be expressed by $z_{1:t} = \{z_1,......, z_t\}$. SLAM can be broadly classified in to 2 types, online-SLAM and full-SLAM.
\\

\textbf{Online-SLAM.} Estimating the current pose and the map.

\begin{equation*}
	\begin{split}
		p(x_t, m | z_{1:t}, u_{1:t})\\	
	\end{split}
	\label{eq:online_slam}
\end{equation*} 

where $x_t$ is the camera pose at current time t, m is the map, $z_{1:t}$ and $u_{1:t}$ are measurement and control inputs until the current time t respectively. Online-SLAM is essentially an online state estimation problem where the state is the current camera pose and the map. The pose and the map is estimated and refined as and when the new measurements are available. Online-SLAM problem is modeled and solved using filtering techniques such as Kalman and information filters \cite{smith1990estimating} \cite{castellanos1999spmap} and particle filters \cite{montemerlo2002fastslam}.
\\

\textbf{Full-SLAM.} Estimating the entire camera trajectory and the map instead of just momentary pose.

\begin{equation*}
	\begin{split}
		p(x_{1:t}, m | z_{1:t}, u_{1:t})\\	
	\end{split}
	\label{eq:full_slam}
\end{equation*} 

Full-SLAM estimates $x_{1:t}$ the entire trajectory of the camera and simultaneously mapping the electrodes by incorporating a full set of measurements and control inputs $z_{1:t}$ and $u_{1:t}$. Smoothing techniques like \cite{lu1997globally} \cite{dellaert2006square} are employed to solve the full-SLAM problems. The modern and intuitive approach for solving a full-SLAM problem is to use graph-based formulation. Due to the improvements in the areas of computational power and efficient linear algebra solvers, graph-based formulations have gained popularity over the years \cite{lu1997globally}.  

\section{Graph-Based SLAM (Pose-Graph SLAM)}
The graph-based approach uses an intuitive graph that consists of nodes and edges to represent the full-SLAM problem. Each node corresponds to the camera pose along the trajectory and an edge between two nodes corresponds to the spatial constraints (odometry/loop closure constraints) between them. These constraints are inherently uncertain in nature. The idea of the graph-based approach is to first construct the pose-graph and determine an optimized node configuration (camera poses) that minimizes the error introduced by spatial constraints. Finally, render a map based on these optimized poses as shown in \cref{fig:graph_based_slam}. The underlying mathematics and equations are referenced from the literature \cite{grisetti2010tutorial}, \cite{miniSam}.

\begin{figure}[hbt!]
	\centering
	\includegraphics[scale=0.5]{graph_based_slam.png}
	\caption{Graph based SLAM in a nutshell.}
	\label{fig:graph_based_slam}
\end{figure}


\subsection{Pose-Graph construction}
The graph consists of n nodes $x_{1:t}$ which corresponds to the camera pose at time t. Every time the camera moves from $x_i$ to $x_{i+1}$ position an edge(odometry) constraint is added to the graph connecting nodes $x_i$ and $x_{i+1}$. If the camera revisits the previously visited position, for example $x_4$, an loop closure constraint is added to the graph connecting the current node $x_t$ and $x_j$ as shown in \cref{fig:graph_construction}.

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\linewidth]{graph_construction.png}
	\caption{Pose-Graph consists of nodes and edges.}
	\label{fig:graph_construction}
\end{figure}


Since spatial constraints associated with edges (sensor measurement of some sort) are inherently noisy, these are modeled probabilistically. Let $Z_{ij}$ and $\Omega$ be mean and information matrix of a measurement (please see appendix on Gaussian distribution) between node $x_i$ and $x_j$. Let $\hat{Z}_{ij}$ the prediction of this measurement given a node configuration $x_i$ and $x_j$ as shown in \cref{fig:error_function}.  

\begin{figure}[hbt!]
	\centering
	\includegraphics[scale=0.5]{error_function.png}
	\caption{Probabilistic model of measurements \cite{grisetti2010tutorial}.}
	\label{fig:error_function}
\end{figure}

The log-likelihood  $l_{ij}$ of a measurement $Z_{ij}$ is therefore,

\begin{equation}
	\begin{split}
		l_{ij} \propto [Z_{ij} - \hat{Z}_{ij}(x_i , x_j)]^T \Omega_{ij} [Z_{ij} - \hat{Z}_{ij}(x_i , x_j)]\\	
	\end{split}
	\label{eq:log-likelihood}
\end{equation}

Let $e_{ij}(x_i, x_j)$ be the function that computes difference between real and expected observation/measurement,

\begin{equation}
	\begin{split}
		e_{ij}(x_i, x_j) = [Z_{ij} - \hat{Z}_{ij}(x_i , x_j)]	
	\end{split}
	\label{eq:error_function}
\end{equation}

Let $\mathcal{C}$ be the set with many nodes for which observation $Z_{ij}$ exists then, the objective of maximum likelihood approach is to find the configuration $X^*$ that minimizes the negative log likelihood of the function $F(x)$ of all observations.
  
\begin{equation*}
	\begin{split}
		F(x) = \sum_{(i,j) \in \mathcal{C}}  e_{ij}^T \Omega_{ij} e_{ij}
	\end{split}
\end{equation*}

which can be rewritten as,
{\scriptsize {\large }}
\begin{equation}
	\begin{split}
		X^* = \underset{x}{\mathrm{argmin}}  \sum_{(i,j) \in \mathcal{C}}  e_{ij}^T \Omega_{ij} e_{ij}
	\end{split}
\label{eq:non_linear_least_square_problem}
\end{equation}

\subsection{Pose-Graph optimization}

The \cref{eq:non_linear_least_square_problem} is a non-linear least square problem and solution can be obtained using Gauss-Newton or Lavenberg-Marquardt algorithms \cite{grisetti2010tutorial}. Dellaert and Kaess \cite{dellaert2006square} have shown the connection between factor graphs and non-linear least square problems. A Factor graphs encodes the probabilistic nature of SLAM. A factor graph represents joint probabilities of all the factors with factor nodes $f_i \in \mathcal{F}$, variable nodes $x_i \in \mathcal{X}$.  

\begin{equation}
	\begin{split}
		g(\mathcal{X}) = \prod_i f_i(\mathcal{X}_i)
	\end{split}
\label{eq:factor_graph}
\end{equation}

each factor $f_i$ consists of measurement function $h_i(\mathcal{X}_i)$ and a measurement $z_i$. Assuming Gaussian measurement models $f_i$ can be given as 

\begin{equation*}
	\begin{split}
		f_i(\mathcal{X}_i) \propto exp (\frac{1}{2} \| h_i(\mathcal{X}_i) - z_i \|_\Sigma^2)
	\end{split}
\end{equation*}

with $\| e \|_\Sigma^2 = e^T\invMat{\Sigma}e$ being the Mahalanobis distance with covariance matrix $\Sigma$. A set of node configurations $\mathcal{X}^*$ that maximizes \cref{eq:factor_graph} can be posed as a nonlinear least square problem.


\begin{equation}
	\begin{split}
		\mathcal{X}^* = \underset{\mathcal{X}}{\mathrm{argmin}}  \ \frac{1}{2} \sum_{(i,j) \in \mathcal{C}}  \| h_i(\mathcal{X}_i) - z_i \|_\Sigma^2
	\end{split}
	\label{eq:factor_graph_as_non_linear_least_square_problem}
\end{equation}

The \cref{fig:graph_construction} describes simple scenario where there are four camera positions $x_{1:4}$ corresponding to the time stamps $t_{1:4}$. A factor graph models the pose-graph as \cref{fig:factor_graph_example}. Prior factor describes prior distribution on the very first pose $x_1$ and encodes the relation with world coordinate frame. Odometry factors encodes the spatial constraint between subsequent nodes  $x_{i}$ with $x_{i+1}$ and loop factor encodes the relative spatial constraint between $x_{4}$ and $x_{1}$.

\begin{figure}[hbt!]
	\centering
	\includegraphics[scale=0.5]{factor_graph_example.png}
	\caption{Pose-Graph as a factor graph model.}
	\label{fig:factor_graph_example}
\end{figure}


\section{Electrode detection and localization}
Electrodes captured in a sequence of RGB images need to be detected and simultaneously localized as shown in \cref{fig:yolo_detection} in order to create 3D point clouds for sequential ICP registration and SLAM. YOLO(You Only Look Once) being one of the most popular convolutional neural network based object detection and localization algorithm has been employed for this very purpose. YOLO has been trained on a large data-set of EEG cap and trained weights are available as a part of the pipeline. The center point of YOLO bounding box is considered as the 2D electrode position.

\begin{figure}[hbt!]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\linewidth]{yolo_detection.jpg}	
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.37\textwidth}
		\centering
		\includegraphics[width=\linewidth]{yolo_detection_2.jpg}	
	\end{subfigure}
	\caption{Electrode detection and localization using YOLO.}
	\label{fig:yolo_detection} 
\end{figure}  

\section{Cluster processing} Two mainly used cluster processing algorithms in this thesis are DBSCAN and K-Means provided by Scikit-learn. 

\subsection{DBSCAN}
DBSCAN is a density based clustering algorithm, it differentiates the clusters based on the high and low density of points. It labels samples(points) that are in the high density as core samples, A cluster, therefore, a collection of many core samples. In this thesis, DBSCAN is used to estimate the number of clusters in the electrode map. \textit{eps} and \textit{min\_samples} are the main arguments required by DBSCAN. \textit{eps} is the maximum distance between two samples to be considered as in the neighborhood of each other and \textit{min\_samples} is the number of samples to be considered as a core point \cite{sklearnc38:online}.

\begingroup\makeatletter\def\@currenvir{verbatim}
\verbatim
DBSCAN(eps, min_samples, ...)
output: labels, n_clusters
\end{verbatim}


\subsection{K-means}
K-Means on the other hand tries to group the samples of equal variance together by minimizing the inertia or within-cluster sum-of-squares. However, K-Means requires number of clusters to be specified before hand. K-Means divides $\mathcal{N}$ samples in to $\mathcal{K}$ disjoint clusters $\mathcal{C}$ where each cluster contains $\mathcal{X}$ samples and described by centroids of the cluster $\mu_j$. K-Means aims to determine the $\mu_j$ that minimizes the inertia defined by  \cref{eq:cluster_center_inertia}. The argument \textit{n\_clusters} is number of clusters to form as well as number of centroids to generate \cite{sklearnc38:online}.

\begin{equation}
	\sum_{i=0}^n \min_{\mu_j \in C}  \|  x_i  -  \mu_j \| ^2
	\label{eq:cluster_center_inertia}
\end{equation}

\begingroup\makeatletter\def\@currenvir{verbatim}
\verbatim
KMeans(n_clusters,...)
output: labels, cluster_centers
\end{verbatim}

